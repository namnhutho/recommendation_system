# main.py
from data.load_data import load_data_from_minio
from model.cosine_similarity import compute_cosine_similarity
from preprocessing.prepare_data import preprocess_data
from model.als_model import train_als_model
from recommend.hybrid_recommend import map_recommendations, build_hybrid
from pyspark.sql.functions import col, isnan, max,row_number, dense_rank, countDistinct
from pyspark.sql.window import Window
from recommend.recommendation_model import load_weights_from_minio, get_hybrid_recommendations
from data.upload_to_minio import upload_df_to_minio, upload_df_to_minio_as_parquet
from config.minio_config import get_minio_client
from recommend.trend_popular import compute_scores, df_pandas
from data.collect_data import collect_data
from flask import Flask, request, jsonify
from kafka import KafkaProducer
import json
import threading
import schedule
import time
import pandas as pd
import io
from routes.kafka_routes import kafka_bp
from routes.recommend_routes import recommend_bp
from routes.anonymous_routes import anonymous_bp

app = Flask(__name__)
app.register_blueprint(kafka_bp)
app.register_blueprint(recommend_bp)
app.register_blueprint(anonymous_bp)

# C·∫•u h√¨nh Kafka Producer
producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# @app.route('/send-kafka', methods=['POST'])
# def send_to_kafka():
#     data = request.get_json()
#     if not data:
#         return jsonify({'error': 'No JSON provided'}), 400

#     producer.send('json-topic', value=data)
#     producer.flush()
#     return jsonify({'status': 'Message sent to Kafka ‚úÖ'}), 200

def run_collect_data():
    collect_data()

def run_training_job():
    print("üõ†Ô∏è B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh CF + CBF + Hybrid...")
    try:
        spark, df_spark = load_data_from_minio()
        cosine_df = compute_cosine_similarity(df_spark)

        (train_data, test_data), user_to_index, index_to_user, product_to_index, index_to_product = preprocess_data(df_spark)
        print("S·ªë user v√† product", len(index_to_user), len(index_to_product))
        cvModel, rmse = train_als_model(train_data, test_data)
        print(f"‚úÖ M√¥ h√¨nh CF hu·∫•n luy·ªán xong v·ªõi RMSE: {rmse}")

        top_k = 10
        recommendations = cvModel.bestModel.recommendForAllUsers(top_k)
        final_recommendations = map_recommendations(recommendations, index_to_user, index_to_product, spark)
        cf_recommendations_spark = spark.createDataFrame(final_recommendations)
        hybrid_df = build_hybrid(cf_recommendations_spark, cosine_df, spark)
        # Ch·ªçn ch·ªâ c√°c c·ªôt user_id, product_id v√† final_score
        hybrid_df = hybrid_df.select("user_id", "product_id", "final_score")
        hybrid_df = hybrid_df.dropDuplicates(["product_id", "final_score"])

        # T·∫°o c·ª≠a s·ªï nh√≥m theo user_id v√† s·∫Øp x·∫øp theo final_score gi·∫£m d·∫ßn
        window_spec = Window.partitionBy("user_id").orderBy(col("final_score").desc())

        # T√≠nh th·ª© h·∫°ng d·ª±a tr√™n final_score
        hybrid_df_with_rank = hybrid_df.withColumn("rank", row_number().over(window_spec))

        # Lo·∫°i b·ªè c√°c product_id tr√πng l·∫∑p trong t·ª´ng nh√≥m user_id
        # Ch·ªâ gi·ªØ l·∫°i product_id ƒë·∫ßu ti√™n xu·∫•t hi·ªán (v·ªõi th·ª© t·ª± rank)
        window_spec_unique = Window.partitionBy("user_id", "product_id").orderBy(col("rank"))
        hybrid_df_unique = hybrid_df_with_rank.withColumn("dense_rank", dense_rank().over(window_spec_unique))

        # L·ªçc gi·ªØ l·∫°i c√°c s·∫£n ph·∫©m v·ªõi dense_rank = 1 (kh√¥ng tr√πng product_id)
        hybrid_df_filtered = hybrid_df_unique.filter(col("dense_rank") == 1)

        # L·∫•y top 10 s·∫£n ph·∫©m cho m·ªói user_id
        window_spec_top10 = Window.partitionBy("user_id").orderBy(col("final_score").desc())
        top10_per_user = hybrid_df_filtered.withColumn("rank", row_number().over(window_spec_top10)) \
                                        .filter(col("rank") <= 10) \
                                        .drop("rank", "dense_rank")  # X√≥a c·ªôt kh√¥ng c·∫ßn thi·∫øt
        # Hi·ªÉn th·ªã k·∫øt qu·∫£
        print("ƒê√É L·∫§Y CH·ªà USER_ID, PRODUCT_ID V√Ä FINAL_SCORE")
        print(top10_per_user)
        t=time.time()
        df_pandas = top10_per_user.toPandas()
        print(time.time()-t)
        print("D·ªØ li·ªáu t·ª´ pandas")
        print(df_pandas)
        # ‚úÖ Upload l√™n MinIO
        upload_df_to_minio(df_pandas, bucket_name='eventsdataset', object_name='recommendations_testtop10v2.csv')
        print("üéâ Hu·∫•n luy·ªán m√¥ h√¨nh ho√†n t·∫•t.")
    except Exception as e:
        print(f"‚ùå L·ªói khi hu·∫•n luy·ªán m√¥ h√¨nh: {e}")
        
def schedule_training():
    # Hu·∫•n luy·ªán m·ªói ng√†y l√∫c 01:00 s√°ng
    schedule.every().day.at("08:39").do(run_training_job)

    # L·∫∑p v√¥ h·∫°n ƒë·ªÉ ch·∫°y l·ªãch
    while True:
        schedule.run_pending()
        time.sleep(60)

def load_csv_from_minio(bucket: str, object_name: str):
    try:
        minio_client = get_minio_client()
        response = minio_client.get_object(bucket, object_name)
        data = response.read()
        return pd.read_csv(io.BytesIO(data))
    except Exception as e:
        print(f"‚ùå L·ªói khi load {object_name} t·ª´ MinIO: {e}")
        return None

# curl http://127.0.0.1:5000/recommend-weighted/1515915625519645266
# T·∫£i m√¥ h√¨nh tr·ªçng s·ªë t·ª´ MinIO khi kh·ªüi ƒë·ªông server
# df_metadata = load_csv_from_minio("eventsdataset", "events.csv")  # Dataset g·ªëc
# df_weight = load_weights_from_minio()
# if df_weight:
#     print("True")
# else:
#     print("False")
# @app.route('/recommend-weighted/<int:user_id>', methods=['GET'])
# def recommend_weighted(user_id):
#     try:
#         top_k = 10
#         recommendations_df = get_hybrid_recommendations(user_id, df_weight, k=top_k)

#         if hasattr(recommendations_df, "toPandas"):
#             recommendations_df = recommendations_df.toPandas()
#         if not recommendations_df.empty:
#             # L·∫•y metadata t·ª´ dataset g·ªëc
#             metadata_df = df_metadata[['product_id', 'category_code', 'brand', 'price']].drop_duplicates()
#             merged_df = recommendations_df.merge(metadata_df, on='product_id', how='left')
#             recommendations_list = merged_df.to_dict(orient='records')
#         else:
#             recommendations_list = []
#         return jsonify({
#             'user_id': user_id,
#             'recommendations': recommendations_list
#         })

#     except Exception as e:
#         return jsonify({'error': str(e)}), 500

# curl "http://localhost:5000/trending-products?top_n=5&recent_days=14"
# curl http://localhost:5000/trending-products
# curl http://localhost:5000/popular-products
@app.route("/popular-products", methods=["GET"])
def get_popular_products():
    popular, _ = compute_scores(df_pandas)
    return jsonify(popular.to_dict(orient="records"))

@app.route("/trending-products", methods=["GET"])
def get_trending_products():
    _, trending = compute_scores(df_pandas)
    return jsonify(trending.to_dict(orient="records"))
    
if __name__ == '__main__':
    # T·∫°o thread ƒë·ªÉ ch·∫°y collect_data song song
    t = threading.Thread(target=run_collect_data, daemon=True)
    t.start()
    # Thread 2: l√™n l·ªãch hu·∫•n luy·ªán ƒë·ªãnh k·ª≥
    t2 = threading.Thread(target=schedule_training, daemon=True)
    t2.start()
    # Kh·ªüi ƒë·ªông Flask server
    print("üöÄ Flask ƒëang ch·∫°y t·∫°i http://localhost:5000")
    app.run(host='0.0.0.0', port=5000)
    
    
    